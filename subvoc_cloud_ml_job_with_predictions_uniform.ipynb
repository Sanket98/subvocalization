{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensorlow      1.12.0\n",
      "numpy          1.15.4\n",
      "\n",
      "init variables\n",
      "define TFR parsers\n",
      "define input functions\n",
      "define model\n",
      "define train function\n",
      "start train...\n",
      "epoch: 1 of 1\n",
      "train: acc=0.905 loss=0.228\n",
      "valid: acc=0.900 loss=0.242\n",
      "train finished\n",
      "start predictions\n",
      "prediction finished\n",
      "create csv file...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "print('\\n{:<15}{}'.format('tensorlow', tf.__version__))\n",
    "\n",
    "\n",
    "\n",
    "print('init variables')\n",
    "\n",
    "# global variables\n",
    "TFR_TRAIN = 'train.tfrecord'\n",
    "TFR_VALID = 'valid.tfrecord'\n",
    "TFR_TEST = 'test.tfrecord'\n",
    "BUCKET = 'gs://robolab/'\n",
    "\n",
    "# image and classes\n",
    "NUM_CLASSES = 2\n",
    "IMG_HEIGHT = 80\n",
    "IMG_WIDTH = 71\n",
    "\n",
    "# model dir\n",
    "OUTDIR = BUCKET + 'output'\n",
    "\n",
    "# hypers\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 1\n",
    "LR = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "print('define TFR parsers')\n",
    "\n",
    "def parser(serialized_example):\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.string)})\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.float32)\n",
    "    image.set_shape([IMG_HEIGHT * IMG_WIDTH])\n",
    "\n",
    "    label = tf.decode_raw(features['label'], tf.int32)\n",
    "    label.set_shape([1])\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def test_parser(serialized_example):\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                 'image_id': tf.FixedLenFeature([], tf.string)})\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.float32)\n",
    "    image.set_shape([IMG_HEIGHT * IMG_WIDTH])\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "print('define input functions')\n",
    "\n",
    "def train_input_fn():\n",
    "\n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_TRAIN)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features, labels = iterator.get_next()\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def eval_input_fn():\n",
    "\n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_TRAIN)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features, labels = iterator.get_next()\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def valid_input_fn():\n",
    "\n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_VALID)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features, labels = iterator.get_next()\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def predict_input_fn():\n",
    "\n",
    "    # get dataset from tf_record\n",
    "    dataset = tf.data.TFRecordDataset(BUCKET + TFR_TEST)\n",
    "\n",
    "    # map parser over dataset samples\n",
    "    dataset = dataset.map(test_parser)\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features = iterator.get_next()\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_image_id(serialized_example):\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                  'image_id': tf.FixedLenFeature([], tf.string)})\n",
    "    \n",
    "    return features['image_id']\n",
    "\n",
    "\n",
    "print('define model')\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    input_layer = tf.reshape(features, [-1, IMG_HEIGHT, IMG_WIDTH])\n",
    "    input_layer = tf.expand_dims(input_layer, axis=3)\n",
    "\n",
    "    conv_layer_1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=8,\n",
    "        kernel_size=[2, 2],\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool_layer_1 = tf.layers.max_pooling2d(\n",
    "        inputs=conv_layer_1,\n",
    "        pool_size=[2, 2],\n",
    "        strides=2,\n",
    "        padding='same')\n",
    "\n",
    "    conv_layer_2 = tf.layers.conv2d(\n",
    "        inputs=pool_layer_1,\n",
    "        filters=32,\n",
    "        kernel_size=[2, 2],\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    pool_layer_2 = tf.layers.max_pooling2d(\n",
    "        inputs=conv_layer_2,\n",
    "        pool_size=[2, 2],\n",
    "        strides=2,\n",
    "        padding='same')\n",
    "\n",
    "    reshape_layer = tf.layers.flatten(pool_layer_2)\n",
    "\n",
    "    dense_layer = tf.layers.dense(\n",
    "        inputs=reshape_layer,\n",
    "        units=256,\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    is_train = False\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        is_train = True\n",
    "\n",
    "    dropout_layer = tf.layers.dropout(\n",
    "        inputs=dense_layer,\n",
    "        rate=0.2,\n",
    "        training=is_train)\n",
    "\n",
    "    logits_layer = tf.layers.dense(\n",
    "        inputs=dropout_layer,\n",
    "        units=NUM_CLASSES)\n",
    "\n",
    "    predictions = {\n",
    "        'classes':tf.argmax(logits_layer, axis=1),\n",
    "        'probabilities':tf.nn.softmax(logits_layer, axis=1)}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels=labels,\n",
    "        logits=logits_layer)\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        labels=labels,\n",
    "        predictions=tf.argmax(logits_layer, axis=1),\n",
    "        name='accu_op')\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        train_optimizer = tf.train.AdamOptimizer(learning_rate=LR).minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_optimizer)\n",
    "\n",
    "    # mode = EVAL\n",
    "    eval_metric_ops = {'accuracy':accuracy}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "\n",
    "print('define train function')\n",
    "\n",
    "def train_and_evaluate(estimator, epochs=1):\n",
    "\n",
    "    all_train_log = []\n",
    "    all_valid_log = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        estimator.train(input_fn=train_input_fn)\n",
    "\n",
    "        train_log = estimator.evaluate(input_fn=eval_input_fn)\n",
    "        print('epoch: {} of {}'.format(epoch + 1, epochs))\n",
    "        print('train: acc={:.3f} loss={:.3f}'.format(train_log['accuracy'], train_log['loss']))\n",
    "        \n",
    "        valid_log = estimator.evaluate(input_fn=valid_input_fn)\n",
    "        print('valid: acc={:.3f} loss={:.3f}'.format(valid_log['accuracy'], valid_log['loss']))\n",
    "\n",
    "        all_train_log.append(train_log)\n",
    "        all_valid_log.append(valid_log)\n",
    "\n",
    "    return all_train_log, all_valid_log\n",
    "\n",
    "\n",
    "\n",
    "print('start train...')\n",
    "\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=OUTDIR)\n",
    "\n",
    "train_log, valid_log = train_and_evaluate(cnn_classifier, epochs=EPOCHS)\n",
    "\n",
    "print('train finished')\n",
    "\n",
    "\n",
    "\n",
    "print('start predictions')\n",
    "\n",
    "predict_generator = cnn_classifier.predict(input_fn=predict_input_fn)\n",
    "\n",
    "preds = []\n",
    "predict_dictlist = []\n",
    "\n",
    "while True:\n",
    "    item = next(predict_generator, None)\n",
    "    if item == None:\n",
    "        break\n",
    "    predict_dictlist.append(item)\n",
    "\n",
    "for i in range(len(predict_dictlist)):\n",
    "    class_ = predict_dictlist[i]['classes']\n",
    "    preds.append(class_)\n",
    "\n",
    "image_ids = []\n",
    "image_id_tensors = []\n",
    "record_iterator = tf.python_io.tf_record_iterator(path=BUCKET + TFR_TEST)\n",
    "\n",
    "for string_record in record_iterator:\n",
    "    image_id_tensors.append(get_image_id(string_record))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "     image_ids = sess.run(image_id_tensors)\n",
    "\n",
    "print('prediction finished')\n",
    "\n",
    "\n",
    "print('create csv file...')\n",
    "\n",
    "sub_filename = 'submission.csv'\n",
    "submission_filepath = os.path.join(BUCKET, 'subs', sub_filename)\n",
    "\n",
    "preds = [i + 1 for i in preds]\n",
    "\n",
    "with file_io.FileIO(submission_filepath, mode='w') as fout:\n",
    "    fout.write('filename,label\\n')\n",
    "    for pred, image_id in zip(preds, image_ids):\n",
    "        image_id = image_id.decode('utf-8')\n",
    "        fout.write('{},{}\\n'.format(image_id, pred))\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
